{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12278989,"sourceType":"datasetVersion","datasetId":7738067}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import CLIPProcessor, CLIPModel\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader\nfrom torch.nn import functional as F\nfrom torch import nn, optim\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:21:28.906182Z","iopub.execute_input":"2025-06-25T09:21:28.906447Z","iopub.status.idle":"2025-06-25T09:21:28.910903Z","shell.execute_reply.started":"2025-06-25T09:21:28.906431Z","shell.execute_reply":"2025-06-25T09:21:28.910262Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"dataset = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\", split=\"train\")\nprint(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:21:30.733188Z","iopub.execute_input":"2025-06-25T09:21:30.733471Z","iopub.status.idle":"2025-06-25T09:21:31.606294Z","shell.execute_reply.started":"2025-06-25T09:21:30.733452Z","shell.execute_reply":"2025-06-25T09:21:31.605636Z"}},"outputs":[{"name":"stdout","text":"{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7FAD87A07ED0>, 'findings': 'The lungs are clear of focal consolidation, pleural effusion or pneumothorax. The heart size is normal. The mediastinal contours are normal. Multiple surgical clips project over the left breast, and old left rib fractures are noted. ', 'impression': 'No acute cardiopulmonary process.'}\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"model_name = \"openai/clip-vit-base-patch32\"\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel = CLIPModel.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:21:35.214664Z","iopub.execute_input":"2025-06-25T09:21:35.215253Z","iopub.status.idle":"2025-06-25T09:21:36.666173Z","shell.execute_reply.started":"2025-06-25T09:21:35.215230Z","shell.execute_reply":"2025-06-25T09:21:36.665577Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"lora_config = LoraConfig(\n    task_type=TaskType.FEATURE_EXTRACTION,\n    inference_mode=False,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\n        \"text_model.encoder.layers.11.self_attn.q_proj\",\n        \"text_model.encoder.layers.11.self_attn.v_proj\",\n        \"vision_model.encoder.layers.11.self_attn.query\",\n        \"vision_model.encoder.layers.11.self_attn.value\"\n    ]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:21:43.348191Z","iopub.execute_input":"2025-06-25T09:21:43.348820Z","iopub.status.idle":"2025-06-25T09:21:43.362327Z","shell.execute_reply.started":"2025-06-25T09:21:43.348797Z","shell.execute_reply":"2025-06-25T09:21:43.361591Z"}},"outputs":[{"name":"stdout","text":"trainable params: 16,384 || all params: 151,293,697 || trainable%: 0.0108\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"named_modules = list(model.named_modules())\nfor i, (name, module) in enumerate(named_modules):\n    print(f\"{i:3d}: {name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:23:06.377990Z","iopub.execute_input":"2025-06-25T09:23:06.378756Z","iopub.status.idle":"2025-06-25T09:23:06.385839Z","shell.execute_reply.started":"2025-06-25T09:23:06.378730Z","shell.execute_reply":"2025-06-25T09:23:06.385199Z"}},"outputs":[{"name":"stdout","text":"  0: \n  1: base_model\n  2: base_model.model\n  3: base_model.model.text_model\n  4: base_model.model.text_model.embeddings\n  5: base_model.model.text_model.embeddings.token_embedding\n  6: base_model.model.text_model.embeddings.position_embedding\n  7: base_model.model.text_model.encoder\n  8: base_model.model.text_model.encoder.layers\n  9: base_model.model.text_model.encoder.layers.0\n 10: base_model.model.text_model.encoder.layers.0.self_attn\n 11: base_model.model.text_model.encoder.layers.0.self_attn.k_proj\n 12: base_model.model.text_model.encoder.layers.0.self_attn.v_proj\n 13: base_model.model.text_model.encoder.layers.0.self_attn.q_proj\n 14: base_model.model.text_model.encoder.layers.0.self_attn.out_proj\n 15: base_model.model.text_model.encoder.layers.0.layer_norm1\n 16: base_model.model.text_model.encoder.layers.0.mlp\n 17: base_model.model.text_model.encoder.layers.0.mlp.activation_fn\n 18: base_model.model.text_model.encoder.layers.0.mlp.fc1\n 19: base_model.model.text_model.encoder.layers.0.mlp.fc2\n 20: base_model.model.text_model.encoder.layers.0.layer_norm2\n 21: base_model.model.text_model.encoder.layers.1\n 22: base_model.model.text_model.encoder.layers.1.self_attn\n 23: base_model.model.text_model.encoder.layers.1.self_attn.k_proj\n 24: base_model.model.text_model.encoder.layers.1.self_attn.v_proj\n 25: base_model.model.text_model.encoder.layers.1.self_attn.q_proj\n 26: base_model.model.text_model.encoder.layers.1.self_attn.out_proj\n 27: base_model.model.text_model.encoder.layers.1.layer_norm1\n 28: base_model.model.text_model.encoder.layers.1.mlp\n 29: base_model.model.text_model.encoder.layers.1.mlp.activation_fn\n 30: base_model.model.text_model.encoder.layers.1.mlp.fc1\n 31: base_model.model.text_model.encoder.layers.1.mlp.fc2\n 32: base_model.model.text_model.encoder.layers.1.layer_norm2\n 33: base_model.model.text_model.encoder.layers.2\n 34: base_model.model.text_model.encoder.layers.2.self_attn\n 35: base_model.model.text_model.encoder.layers.2.self_attn.k_proj\n 36: base_model.model.text_model.encoder.layers.2.self_attn.v_proj\n 37: base_model.model.text_model.encoder.layers.2.self_attn.q_proj\n 38: base_model.model.text_model.encoder.layers.2.self_attn.out_proj\n 39: base_model.model.text_model.encoder.layers.2.layer_norm1\n 40: base_model.model.text_model.encoder.layers.2.mlp\n 41: base_model.model.text_model.encoder.layers.2.mlp.activation_fn\n 42: base_model.model.text_model.encoder.layers.2.mlp.fc1\n 43: base_model.model.text_model.encoder.layers.2.mlp.fc2\n 44: base_model.model.text_model.encoder.layers.2.layer_norm2\n 45: base_model.model.text_model.encoder.layers.3\n 46: base_model.model.text_model.encoder.layers.3.self_attn\n 47: base_model.model.text_model.encoder.layers.3.self_attn.k_proj\n 48: base_model.model.text_model.encoder.layers.3.self_attn.v_proj\n 49: base_model.model.text_model.encoder.layers.3.self_attn.q_proj\n 50: base_model.model.text_model.encoder.layers.3.self_attn.out_proj\n 51: base_model.model.text_model.encoder.layers.3.layer_norm1\n 52: base_model.model.text_model.encoder.layers.3.mlp\n 53: base_model.model.text_model.encoder.layers.3.mlp.activation_fn\n 54: base_model.model.text_model.encoder.layers.3.mlp.fc1\n 55: base_model.model.text_model.encoder.layers.3.mlp.fc2\n 56: base_model.model.text_model.encoder.layers.3.layer_norm2\n 57: base_model.model.text_model.encoder.layers.4\n 58: base_model.model.text_model.encoder.layers.4.self_attn\n 59: base_model.model.text_model.encoder.layers.4.self_attn.k_proj\n 60: base_model.model.text_model.encoder.layers.4.self_attn.v_proj\n 61: base_model.model.text_model.encoder.layers.4.self_attn.q_proj\n 62: base_model.model.text_model.encoder.layers.4.self_attn.out_proj\n 63: base_model.model.text_model.encoder.layers.4.layer_norm1\n 64: base_model.model.text_model.encoder.layers.4.mlp\n 65: base_model.model.text_model.encoder.layers.4.mlp.activation_fn\n 66: base_model.model.text_model.encoder.layers.4.mlp.fc1\n 67: base_model.model.text_model.encoder.layers.4.mlp.fc2\n 68: base_model.model.text_model.encoder.layers.4.layer_norm2\n 69: base_model.model.text_model.encoder.layers.5\n 70: base_model.model.text_model.encoder.layers.5.self_attn\n 71: base_model.model.text_model.encoder.layers.5.self_attn.k_proj\n 72: base_model.model.text_model.encoder.layers.5.self_attn.v_proj\n 73: base_model.model.text_model.encoder.layers.5.self_attn.q_proj\n 74: base_model.model.text_model.encoder.layers.5.self_attn.out_proj\n 75: base_model.model.text_model.encoder.layers.5.layer_norm1\n 76: base_model.model.text_model.encoder.layers.5.mlp\n 77: base_model.model.text_model.encoder.layers.5.mlp.activation_fn\n 78: base_model.model.text_model.encoder.layers.5.mlp.fc1\n 79: base_model.model.text_model.encoder.layers.5.mlp.fc2\n 80: base_model.model.text_model.encoder.layers.5.layer_norm2\n 81: base_model.model.text_model.encoder.layers.6\n 82: base_model.model.text_model.encoder.layers.6.self_attn\n 83: base_model.model.text_model.encoder.layers.6.self_attn.k_proj\n 84: base_model.model.text_model.encoder.layers.6.self_attn.v_proj\n 85: base_model.model.text_model.encoder.layers.6.self_attn.q_proj\n 86: base_model.model.text_model.encoder.layers.6.self_attn.out_proj\n 87: base_model.model.text_model.encoder.layers.6.layer_norm1\n 88: base_model.model.text_model.encoder.layers.6.mlp\n 89: base_model.model.text_model.encoder.layers.6.mlp.activation_fn\n 90: base_model.model.text_model.encoder.layers.6.mlp.fc1\n 91: base_model.model.text_model.encoder.layers.6.mlp.fc2\n 92: base_model.model.text_model.encoder.layers.6.layer_norm2\n 93: base_model.model.text_model.encoder.layers.7\n 94: base_model.model.text_model.encoder.layers.7.self_attn\n 95: base_model.model.text_model.encoder.layers.7.self_attn.k_proj\n 96: base_model.model.text_model.encoder.layers.7.self_attn.v_proj\n 97: base_model.model.text_model.encoder.layers.7.self_attn.q_proj\n 98: base_model.model.text_model.encoder.layers.7.self_attn.out_proj\n 99: base_model.model.text_model.encoder.layers.7.layer_norm1\n100: base_model.model.text_model.encoder.layers.7.mlp\n101: base_model.model.text_model.encoder.layers.7.mlp.activation_fn\n102: base_model.model.text_model.encoder.layers.7.mlp.fc1\n103: base_model.model.text_model.encoder.layers.7.mlp.fc2\n104: base_model.model.text_model.encoder.layers.7.layer_norm2\n105: base_model.model.text_model.encoder.layers.8\n106: base_model.model.text_model.encoder.layers.8.self_attn\n107: base_model.model.text_model.encoder.layers.8.self_attn.k_proj\n108: base_model.model.text_model.encoder.layers.8.self_attn.v_proj\n109: base_model.model.text_model.encoder.layers.8.self_attn.q_proj\n110: base_model.model.text_model.encoder.layers.8.self_attn.out_proj\n111: base_model.model.text_model.encoder.layers.8.layer_norm1\n112: base_model.model.text_model.encoder.layers.8.mlp\n113: base_model.model.text_model.encoder.layers.8.mlp.activation_fn\n114: base_model.model.text_model.encoder.layers.8.mlp.fc1\n115: base_model.model.text_model.encoder.layers.8.mlp.fc2\n116: base_model.model.text_model.encoder.layers.8.layer_norm2\n117: base_model.model.text_model.encoder.layers.9\n118: base_model.model.text_model.encoder.layers.9.self_attn\n119: base_model.model.text_model.encoder.layers.9.self_attn.k_proj\n120: base_model.model.text_model.encoder.layers.9.self_attn.v_proj\n121: base_model.model.text_model.encoder.layers.9.self_attn.q_proj\n122: base_model.model.text_model.encoder.layers.9.self_attn.out_proj\n123: base_model.model.text_model.encoder.layers.9.layer_norm1\n124: base_model.model.text_model.encoder.layers.9.mlp\n125: base_model.model.text_model.encoder.layers.9.mlp.activation_fn\n126: base_model.model.text_model.encoder.layers.9.mlp.fc1\n127: base_model.model.text_model.encoder.layers.9.mlp.fc2\n128: base_model.model.text_model.encoder.layers.9.layer_norm2\n129: base_model.model.text_model.encoder.layers.10\n130: base_model.model.text_model.encoder.layers.10.self_attn\n131: base_model.model.text_model.encoder.layers.10.self_attn.k_proj\n132: base_model.model.text_model.encoder.layers.10.self_attn.v_proj\n133: base_model.model.text_model.encoder.layers.10.self_attn.q_proj\n134: base_model.model.text_model.encoder.layers.10.self_attn.out_proj\n135: base_model.model.text_model.encoder.layers.10.layer_norm1\n136: base_model.model.text_model.encoder.layers.10.mlp\n137: base_model.model.text_model.encoder.layers.10.mlp.activation_fn\n138: base_model.model.text_model.encoder.layers.10.mlp.fc1\n139: base_model.model.text_model.encoder.layers.10.mlp.fc2\n140: base_model.model.text_model.encoder.layers.10.layer_norm2\n141: base_model.model.text_model.encoder.layers.11\n142: base_model.model.text_model.encoder.layers.11.self_attn\n143: base_model.model.text_model.encoder.layers.11.self_attn.k_proj\n144: base_model.model.text_model.encoder.layers.11.self_attn.v_proj\n145: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.base_layer\n146: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_dropout\n147: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_dropout.default\n148: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A\n149: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_A.default\n150: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B\n151: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_B.default\n152: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_embedding_A\n153: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_embedding_B\n154: base_model.model.text_model.encoder.layers.11.self_attn.v_proj.lora_magnitude_vector\n155: base_model.model.text_model.encoder.layers.11.self_attn.q_proj\n156: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.base_layer\n157: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_dropout\n158: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_dropout.default\n159: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A\n160: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_A.default\n161: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B\n162: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_B.default\n163: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_embedding_A\n164: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_embedding_B\n165: base_model.model.text_model.encoder.layers.11.self_attn.q_proj.lora_magnitude_vector\n166: base_model.model.text_model.encoder.layers.11.self_attn.out_proj\n167: base_model.model.text_model.encoder.layers.11.layer_norm1\n168: base_model.model.text_model.encoder.layers.11.mlp\n169: base_model.model.text_model.encoder.layers.11.mlp.activation_fn\n170: base_model.model.text_model.encoder.layers.11.mlp.fc1\n171: base_model.model.text_model.encoder.layers.11.mlp.fc2\n172: base_model.model.text_model.encoder.layers.11.layer_norm2\n173: base_model.model.text_model.final_layer_norm\n174: base_model.model.vision_model\n175: base_model.model.vision_model.embeddings\n176: base_model.model.vision_model.embeddings.patch_embedding\n177: base_model.model.vision_model.embeddings.position_embedding\n178: base_model.model.vision_model.pre_layrnorm\n179: base_model.model.vision_model.encoder\n180: base_model.model.vision_model.encoder.layers\n181: base_model.model.vision_model.encoder.layers.0\n182: base_model.model.vision_model.encoder.layers.0.self_attn\n183: base_model.model.vision_model.encoder.layers.0.self_attn.k_proj\n184: base_model.model.vision_model.encoder.layers.0.self_attn.v_proj\n185: base_model.model.vision_model.encoder.layers.0.self_attn.q_proj\n186: base_model.model.vision_model.encoder.layers.0.self_attn.out_proj\n187: base_model.model.vision_model.encoder.layers.0.layer_norm1\n188: base_model.model.vision_model.encoder.layers.0.mlp\n189: base_model.model.vision_model.encoder.layers.0.mlp.activation_fn\n190: base_model.model.vision_model.encoder.layers.0.mlp.fc1\n191: base_model.model.vision_model.encoder.layers.0.mlp.fc2\n192: base_model.model.vision_model.encoder.layers.0.layer_norm2\n193: base_model.model.vision_model.encoder.layers.1\n194: base_model.model.vision_model.encoder.layers.1.self_attn\n195: base_model.model.vision_model.encoder.layers.1.self_attn.k_proj\n196: base_model.model.vision_model.encoder.layers.1.self_attn.v_proj\n197: base_model.model.vision_model.encoder.layers.1.self_attn.q_proj\n198: base_model.model.vision_model.encoder.layers.1.self_attn.out_proj\n199: base_model.model.vision_model.encoder.layers.1.layer_norm1\n200: base_model.model.vision_model.encoder.layers.1.mlp\n201: base_model.model.vision_model.encoder.layers.1.mlp.activation_fn\n202: base_model.model.vision_model.encoder.layers.1.mlp.fc1\n203: base_model.model.vision_model.encoder.layers.1.mlp.fc2\n204: base_model.model.vision_model.encoder.layers.1.layer_norm2\n205: base_model.model.vision_model.encoder.layers.2\n206: base_model.model.vision_model.encoder.layers.2.self_attn\n207: base_model.model.vision_model.encoder.layers.2.self_attn.k_proj\n208: base_model.model.vision_model.encoder.layers.2.self_attn.v_proj\n209: base_model.model.vision_model.encoder.layers.2.self_attn.q_proj\n210: base_model.model.vision_model.encoder.layers.2.self_attn.out_proj\n211: base_model.model.vision_model.encoder.layers.2.layer_norm1\n212: base_model.model.vision_model.encoder.layers.2.mlp\n213: base_model.model.vision_model.encoder.layers.2.mlp.activation_fn\n214: base_model.model.vision_model.encoder.layers.2.mlp.fc1\n215: base_model.model.vision_model.encoder.layers.2.mlp.fc2\n216: base_model.model.vision_model.encoder.layers.2.layer_norm2\n217: base_model.model.vision_model.encoder.layers.3\n218: base_model.model.vision_model.encoder.layers.3.self_attn\n219: base_model.model.vision_model.encoder.layers.3.self_attn.k_proj\n220: base_model.model.vision_model.encoder.layers.3.self_attn.v_proj\n221: base_model.model.vision_model.encoder.layers.3.self_attn.q_proj\n222: base_model.model.vision_model.encoder.layers.3.self_attn.out_proj\n223: base_model.model.vision_model.encoder.layers.3.layer_norm1\n224: base_model.model.vision_model.encoder.layers.3.mlp\n225: base_model.model.vision_model.encoder.layers.3.mlp.activation_fn\n226: base_model.model.vision_model.encoder.layers.3.mlp.fc1\n227: base_model.model.vision_model.encoder.layers.3.mlp.fc2\n228: base_model.model.vision_model.encoder.layers.3.layer_norm2\n229: base_model.model.vision_model.encoder.layers.4\n230: base_model.model.vision_model.encoder.layers.4.self_attn\n231: base_model.model.vision_model.encoder.layers.4.self_attn.k_proj\n232: base_model.model.vision_model.encoder.layers.4.self_attn.v_proj\n233: base_model.model.vision_model.encoder.layers.4.self_attn.q_proj\n234: base_model.model.vision_model.encoder.layers.4.self_attn.out_proj\n235: base_model.model.vision_model.encoder.layers.4.layer_norm1\n236: base_model.model.vision_model.encoder.layers.4.mlp\n237: base_model.model.vision_model.encoder.layers.4.mlp.activation_fn\n238: base_model.model.vision_model.encoder.layers.4.mlp.fc1\n239: base_model.model.vision_model.encoder.layers.4.mlp.fc2\n240: base_model.model.vision_model.encoder.layers.4.layer_norm2\n241: base_model.model.vision_model.encoder.layers.5\n242: base_model.model.vision_model.encoder.layers.5.self_attn\n243: base_model.model.vision_model.encoder.layers.5.self_attn.k_proj\n244: base_model.model.vision_model.encoder.layers.5.self_attn.v_proj\n245: base_model.model.vision_model.encoder.layers.5.self_attn.q_proj\n246: base_model.model.vision_model.encoder.layers.5.self_attn.out_proj\n247: base_model.model.vision_model.encoder.layers.5.layer_norm1\n248: base_model.model.vision_model.encoder.layers.5.mlp\n249: base_model.model.vision_model.encoder.layers.5.mlp.activation_fn\n250: base_model.model.vision_model.encoder.layers.5.mlp.fc1\n251: base_model.model.vision_model.encoder.layers.5.mlp.fc2\n252: base_model.model.vision_model.encoder.layers.5.layer_norm2\n253: base_model.model.vision_model.encoder.layers.6\n254: base_model.model.vision_model.encoder.layers.6.self_attn\n255: base_model.model.vision_model.encoder.layers.6.self_attn.k_proj\n256: base_model.model.vision_model.encoder.layers.6.self_attn.v_proj\n257: base_model.model.vision_model.encoder.layers.6.self_attn.q_proj\n258: base_model.model.vision_model.encoder.layers.6.self_attn.out_proj\n259: base_model.model.vision_model.encoder.layers.6.layer_norm1\n260: base_model.model.vision_model.encoder.layers.6.mlp\n261: base_model.model.vision_model.encoder.layers.6.mlp.activation_fn\n262: base_model.model.vision_model.encoder.layers.6.mlp.fc1\n263: base_model.model.vision_model.encoder.layers.6.mlp.fc2\n264: base_model.model.vision_model.encoder.layers.6.layer_norm2\n265: base_model.model.vision_model.encoder.layers.7\n266: base_model.model.vision_model.encoder.layers.7.self_attn\n267: base_model.model.vision_model.encoder.layers.7.self_attn.k_proj\n268: base_model.model.vision_model.encoder.layers.7.self_attn.v_proj\n269: base_model.model.vision_model.encoder.layers.7.self_attn.q_proj\n270: base_model.model.vision_model.encoder.layers.7.self_attn.out_proj\n271: base_model.model.vision_model.encoder.layers.7.layer_norm1\n272: base_model.model.vision_model.encoder.layers.7.mlp\n273: base_model.model.vision_model.encoder.layers.7.mlp.activation_fn\n274: base_model.model.vision_model.encoder.layers.7.mlp.fc1\n275: base_model.model.vision_model.encoder.layers.7.mlp.fc2\n276: base_model.model.vision_model.encoder.layers.7.layer_norm2\n277: base_model.model.vision_model.encoder.layers.8\n278: base_model.model.vision_model.encoder.layers.8.self_attn\n279: base_model.model.vision_model.encoder.layers.8.self_attn.k_proj\n280: base_model.model.vision_model.encoder.layers.8.self_attn.v_proj\n281: base_model.model.vision_model.encoder.layers.8.self_attn.q_proj\n282: base_model.model.vision_model.encoder.layers.8.self_attn.out_proj\n283: base_model.model.vision_model.encoder.layers.8.layer_norm1\n284: base_model.model.vision_model.encoder.layers.8.mlp\n285: base_model.model.vision_model.encoder.layers.8.mlp.activation_fn\n286: base_model.model.vision_model.encoder.layers.8.mlp.fc1\n287: base_model.model.vision_model.encoder.layers.8.mlp.fc2\n288: base_model.model.vision_model.encoder.layers.8.layer_norm2\n289: base_model.model.vision_model.encoder.layers.9\n290: base_model.model.vision_model.encoder.layers.9.self_attn\n291: base_model.model.vision_model.encoder.layers.9.self_attn.k_proj\n292: base_model.model.vision_model.encoder.layers.9.self_attn.v_proj\n293: base_model.model.vision_model.encoder.layers.9.self_attn.q_proj\n294: base_model.model.vision_model.encoder.layers.9.self_attn.out_proj\n295: base_model.model.vision_model.encoder.layers.9.layer_norm1\n296: base_model.model.vision_model.encoder.layers.9.mlp\n297: base_model.model.vision_model.encoder.layers.9.mlp.activation_fn\n298: base_model.model.vision_model.encoder.layers.9.mlp.fc1\n299: base_model.model.vision_model.encoder.layers.9.mlp.fc2\n300: base_model.model.vision_model.encoder.layers.9.layer_norm2\n301: base_model.model.vision_model.encoder.layers.10\n302: base_model.model.vision_model.encoder.layers.10.self_attn\n303: base_model.model.vision_model.encoder.layers.10.self_attn.k_proj\n304: base_model.model.vision_model.encoder.layers.10.self_attn.v_proj\n305: base_model.model.vision_model.encoder.layers.10.self_attn.q_proj\n306: base_model.model.vision_model.encoder.layers.10.self_attn.out_proj\n307: base_model.model.vision_model.encoder.layers.10.layer_norm1\n308: base_model.model.vision_model.encoder.layers.10.mlp\n309: base_model.model.vision_model.encoder.layers.10.mlp.activation_fn\n310: base_model.model.vision_model.encoder.layers.10.mlp.fc1\n311: base_model.model.vision_model.encoder.layers.10.mlp.fc2\n312: base_model.model.vision_model.encoder.layers.10.layer_norm2\n313: base_model.model.vision_model.encoder.layers.11\n314: base_model.model.vision_model.encoder.layers.11.self_attn\n315: base_model.model.vision_model.encoder.layers.11.self_attn.k_proj\n316: base_model.model.vision_model.encoder.layers.11.self_attn.v_proj\n317: base_model.model.vision_model.encoder.layers.11.self_attn.q_proj\n318: base_model.model.vision_model.encoder.layers.11.self_attn.out_proj\n319: base_model.model.vision_model.encoder.layers.11.layer_norm1\n320: base_model.model.vision_model.encoder.layers.11.mlp\n321: base_model.model.vision_model.encoder.layers.11.mlp.activation_fn\n322: base_model.model.vision_model.encoder.layers.11.mlp.fc1\n323: base_model.model.vision_model.encoder.layers.11.mlp.fc2\n324: base_model.model.vision_model.encoder.layers.11.layer_norm2\n325: base_model.model.vision_model.post_layernorm\n326: base_model.model.visual_projection\n327: base_model.model.text_projection\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"class CLIPDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.ds = dataset\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        return {\n            \"text\": item[\"findings\"],\n            \"image\": item[\"image\"]\n        }\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:23:26.649909Z","iopub.execute_input":"2025-06-25T09:23:26.650484Z","iopub.status.idle":"2025-06-25T09:23:26.654821Z","shell.execute_reply.started":"2025-06-25T09:23:26.650462Z","shell.execute_reply":"2025-06-25T09:23:26.654023Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def collate_fn(batch):\n    texts = [item[\"text\"] for item in batch]\n    images = [item[\"image\"] for item in batch]\n    \n    inputs = processor(\n        text=texts,\n        images=images,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=77\n    )\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:23:32.119542Z","iopub.execute_input":"2025-06-25T09:23:32.120087Z","iopub.status.idle":"2025-06-25T09:23:32.124170Z","shell.execute_reply.started":"2025-06-25T09:23:32.120065Z","shell.execute_reply":"2025-06-25T09:23:32.123465Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"train_dataset = CLIPDataset(dataset)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\ndef contrastive_loss(logits_per_image, logits_per_text, temperature=0.07):\n    batch_size = logits_per_image.shape[0]\n    labels = torch.arange(batch_size, device=logits_per_image.device)\n    \n    logits_per_image = logits_per_image / temperature\n    logits_per_text = logits_per_text / temperature\n    \n    loss_img = F.cross_entropy(logits_per_image, labels)\n    loss_txt = F.cross_entropy(logits_per_text, labels)\n    \n    return (loss_img + loss_txt) / 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:24:52.756961Z","iopub.execute_input":"2025-06-25T09:24:52.757598Z","iopub.status.idle":"2025-06-25T09:24:52.763453Z","shell.execute_reply.started":"2025-06-25T09:24:52.757573Z","shell.execute_reply":"2025-06-25T09:24:52.762540Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"def safe_forward(model, input_ids, attention_mask, pixel_values):\n    if hasattr(model, 'base_model'):\n        base_model = model.base_model\n    else:\n        base_model = model\n        \n    text_outputs = base_model.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    \n    vision_outputs = base_model.vision_model(pixel_values=pixel_values)\n    text_embeds = text_outputs.pooler_output\n    image_embeds = vision_outputs.pooler_output\n    text_embeds = base_model.text_projection(text_embeds)\n    image_embeds = base_model.visual_projection(image_embeds)\n\n    class CLIPOutput:\n        def __init__(self, text_embeds, image_embeds):\n            self.text_embeds = text_embeds\n            self.image_embeds = image_embeds\n    \n    return CLIPOutput(text_embeds, image_embeds)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:25:11.211983Z","iopub.execute_input":"2025-06-25T09:25:11.212637Z","iopub.status.idle":"2025-06-25T09:25:11.456931Z","shell.execute_reply.started":"2025-06-25T09:25:11.212602Z","shell.execute_reply":"2025-06-25T09:25:11.456164Z"}},"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"PeftModelForFeatureExtraction(\n  (base_model): LoraModel(\n    (model): CLIPModel(\n      (text_model): CLIPTextTransformer(\n        (embeddings): CLIPTextEmbeddings(\n          (token_embedding): Embedding(49408, 512)\n          (position_embedding): Embedding(77, 512)\n        )\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-10): 11 x CLIPEncoderLayer(\n              (self_attn): CLIPSdpaAttention(\n                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n              )\n              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n            (11): CLIPEncoderLayer(\n              (self_attn): CLIPSdpaAttention(\n                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=512, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n              )\n              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n              )\n              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n          (position_embedding): Embedding(50, 768)\n        )\n        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x CLIPEncoderLayer(\n              (self_attn): CLIPSdpaAttention(\n                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n      (text_projection): Linear(in_features=512, out_features=512, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":94},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=1e-4)\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    num_batches = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        outputs = safe_forward(model, input_ids, attention_mask, pixel_values)\n        image_embeds = outputs.image_embeds\n        text_embeds = outputs.text_embeds\n\n        image_embeds = F.normalize(image_embeds, p=2, dim=1)\n        text_embeds = F.normalize(text_embeds, p=2, dim=1)\n        \n        logits_per_image = torch.matmul(image_embeds, text_embeds.t())\n        logits_per_text = torch.matmul(text_embeds, image_embeds.t())\n        \n        loss = contrastive_loss(logits_per_image, logits_per_text)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n        \n        if num_batches % 100 == 0:\n            avg_loss = total_loss / num_batches\n            print(f\"Batch {num_batches}, Average Loss: {avg_loss:.4f}\")\n    \n    if num_batches > 0:\n        avg_epoch_loss = total_loss / num_batches\n        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_epoch_loss:.4f}\")\n    else:\n        print(f\"Epoch {epoch+1} completed with no successful batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:25:56.004482Z","iopub.execute_input":"2025-06-25T09:25:56.005168Z","iopub.status.idle":"2025-06-25T09:32:52.733641Z","shell.execute_reply.started":"2025-06-25T09:25:56.005144Z","shell.execute_reply":"2025-06-25T09:32:52.732844Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/1:   1%|▏         | 102/7659 [00:05<06:53, 18.28it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 100, Average Loss: 1.3993\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   3%|▎         | 202/7659 [00:11<06:41, 18.56it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 200, Average Loss: 1.4009\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   4%|▍         | 302/7659 [00:16<06:42, 18.27it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 300, Average Loss: 1.3962\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   5%|▌         | 402/7659 [00:21<06:45, 17.89it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 400, Average Loss: 1.3947\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   7%|▋         | 502/7659 [00:27<06:27, 18.46it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 500, Average Loss: 1.3926\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   8%|▊         | 602/7659 [00:33<06:21, 18.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 600, Average Loss: 1.3913\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:   9%|▉         | 702/7659 [00:38<06:26, 18.01it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 700, Average Loss: 1.3904\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  10%|█         | 798/7659 [00:43<06:12, 18.42it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 800, Average Loss: 1.3902\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  12%|█▏        | 902/7659 [00:49<06:08, 18.31it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 900, Average Loss: 1.3887\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  13%|█▎        | 1002/7659 [00:54<05:59, 18.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1000, Average Loss: 1.3875\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  14%|█▍        | 1102/7659 [01:00<06:40, 16.37it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1100, Average Loss: 1.3860\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  16%|█▌        | 1202/7659 [01:05<05:48, 18.54it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1200, Average Loss: 1.3843\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  17%|█▋        | 1302/7659 [01:11<05:43, 18.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1300, Average Loss: 1.3831\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  18%|█▊        | 1402/7659 [01:16<05:37, 18.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1400, Average Loss: 1.3811\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  20%|█▉        | 1502/7659 [01:21<05:43, 17.95it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1500, Average Loss: 1.3791\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  21%|██        | 1602/7659 [01:27<05:26, 18.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1600, Average Loss: 1.3766\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  22%|██▏       | 1702/7659 [01:32<05:21, 18.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1700, Average Loss: 1.3748\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  24%|██▎       | 1802/7659 [01:38<05:41, 17.14it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1800, Average Loss: 1.3730\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  25%|██▍       | 1902/7659 [01:43<05:16, 18.22it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 1900, Average Loss: 1.3713\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  26%|██▌       | 2002/7659 [01:49<05:00, 18.85it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2000, Average Loss: 1.3689\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  27%|██▋       | 2102/7659 [01:54<05:00, 18.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2100, Average Loss: 1.3669\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  29%|██▉       | 2202/7659 [02:00<04:52, 18.69it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2200, Average Loss: 1.3640\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  30%|███       | 2302/7659 [02:05<04:47, 18.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2300, Average Loss: 1.3623\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  31%|███▏      | 2402/7659 [02:10<04:43, 18.56it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2400, Average Loss: 1.3603\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  33%|███▎      | 2502/7659 [02:16<04:38, 18.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2500, Average Loss: 1.3588\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  34%|███▍      | 2602/7659 [02:21<04:33, 18.51it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2600, Average Loss: 1.3570\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  35%|███▌      | 2702/7659 [02:27<04:25, 18.70it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2700, Average Loss: 1.3563\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  37%|███▋      | 2802/7659 [02:32<04:20, 18.61it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2800, Average Loss: 1.3550\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  38%|███▊      | 2902/7659 [02:38<04:34, 17.32it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 2900, Average Loss: 1.3538\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  39%|███▉      | 3002/7659 [02:43<04:13, 18.34it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3000, Average Loss: 1.3528\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  41%|████      | 3102/7659 [02:49<04:05, 18.56it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3100, Average Loss: 1.3519\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  42%|████▏     | 3202/7659 [02:54<04:00, 18.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3200, Average Loss: 1.3512\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  43%|████▎     | 3302/7659 [02:59<03:55, 18.48it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3300, Average Loss: 1.3491\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  44%|████▍     | 3402/7659 [03:05<03:48, 18.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3400, Average Loss: 1.3486\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  46%|████▌     | 3502/7659 [03:10<03:44, 18.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3500, Average Loss: 1.3475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  47%|████▋     | 3602/7659 [03:16<03:38, 18.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3600, Average Loss: 1.3471\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  48%|████▊     | 3702/7659 [03:21<03:33, 18.51it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3700, Average Loss: 1.3469\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  50%|████▉     | 3802/7659 [03:27<03:28, 18.54it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3800, Average Loss: 1.3458\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  51%|█████     | 3902/7659 [03:32<03:21, 18.61it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 3900, Average Loss: 1.3443\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  52%|█████▏    | 4002/7659 [03:38<03:41, 16.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4000, Average Loss: 1.3435\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  54%|█████▎    | 4102/7659 [03:43<03:14, 18.31it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4100, Average Loss: 1.3430\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  55%|█████▍    | 4202/7659 [03:48<03:08, 18.34it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4200, Average Loss: 1.3429\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  56%|█████▌    | 4302/7659 [03:54<03:07, 17.89it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4300, Average Loss: 1.3418\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  57%|█████▋    | 4402/7659 [03:59<02:56, 18.50it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4400, Average Loss: 1.3408\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  59%|█████▉    | 4502/7659 [04:05<02:49, 18.63it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4500, Average Loss: 1.3399\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  60%|██████    | 4602/7659 [04:10<02:50, 17.96it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4600, Average Loss: 1.3392\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  61%|██████▏   | 4702/7659 [04:16<02:38, 18.69it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4700, Average Loss: 1.3381\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  63%|██████▎   | 4802/7659 [04:21<02:32, 18.75it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4800, Average Loss: 1.3376\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  64%|██████▍   | 4902/7659 [04:26<02:30, 18.35it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 4900, Average Loss: 1.3373\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  65%|██████▌   | 5002/7659 [04:32<02:23, 18.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5000, Average Loss: 1.3365\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  67%|██████▋   | 5102/7659 [04:37<02:17, 18.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5100, Average Loss: 1.3359\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  68%|██████▊   | 5202/7659 [04:43<02:11, 18.75it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5200, Average Loss: 1.3350\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  69%|██████▉   | 5302/7659 [04:48<02:10, 18.04it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5300, Average Loss: 1.3340\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  71%|███████   | 5402/7659 [04:54<02:01, 18.65it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5400, Average Loss: 1.3334\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  72%|███████▏  | 5502/7659 [04:59<01:55, 18.64it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5500, Average Loss: 1.3325\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  73%|███████▎  | 5602/7659 [05:04<01:52, 18.25it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5600, Average Loss: 1.3317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  74%|███████▍  | 5702/7659 [05:10<01:44, 18.77it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5700, Average Loss: 1.3309\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  76%|███████▌  | 5802/7659 [05:15<01:39, 18.71it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5800, Average Loss: 1.3300\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  77%|███████▋  | 5902/7659 [05:21<01:33, 18.74it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 5900, Average Loss: 1.3299\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  78%|███████▊  | 6002/7659 [05:26<01:29, 18.62it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6000, Average Loss: 1.3294\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  80%|███████▉  | 6102/7659 [05:32<01:24, 18.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6100, Average Loss: 1.3287\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  81%|████████  | 6202/7659 [05:37<01:20, 18.07it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6200, Average Loss: 1.3282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  82%|████████▏ | 6302/7659 [05:42<01:13, 18.42it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6300, Average Loss: 1.3275\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  84%|████████▎ | 6402/7659 [05:48<01:09, 18.04it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6400, Average Loss: 1.3267\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  85%|████████▍ | 6502/7659 [05:53<01:02, 18.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6500, Average Loss: 1.3262\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  86%|████████▌ | 6602/7659 [05:59<00:57, 18.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6600, Average Loss: 1.3259\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  88%|████████▊ | 6702/7659 [06:04<00:51, 18.42it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6700, Average Loss: 1.3254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  89%|████████▉ | 6802/7659 [06:10<00:45, 18.79it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6800, Average Loss: 1.3250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  90%|█████████ | 6902/7659 [06:15<00:40, 18.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 6900, Average Loss: 1.3246\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  91%|█████████▏| 7002/7659 [06:21<00:34, 18.84it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7000, Average Loss: 1.3240\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  93%|█████████▎| 7102/7659 [06:26<00:29, 18.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7100, Average Loss: 1.3235\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  94%|█████████▍| 7202/7659 [06:31<00:24, 18.45it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7200, Average Loss: 1.3228\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  95%|█████████▌| 7302/7659 [06:37<00:19, 18.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7300, Average Loss: 1.3223\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  97%|█████████▋| 7402/7659 [06:42<00:13, 18.65it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7400, Average Loss: 1.3218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  98%|█████████▊| 7502/7659 [06:48<00:10, 14.80it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7500, Average Loss: 1.3210\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  99%|█████████▉| 7602/7659 [06:53<00:03, 18.51it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 7600, Average Loss: 1.3204\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 7659/7659 [06:56<00:00, 18.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed. Average Loss: 1.3200\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"model.save_pretrained(\"./clip-lora-mimic-cxr\")\nprocessor.save_pretrained(\"./clip-lora-mimic-cxr\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:33:12.588164Z","iopub.execute_input":"2025-06-25T09:33:12.588450Z","iopub.status.idle":"2025-06-25T09:33:12.877051Z","shell.execute_reply.started":"2025-06-25T09:33:12.588428Z","shell.execute_reply":"2025-06-25T09:33:12.876456Z"}},"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom PIL import Image\nfrom torch.nn import functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprocessor = CLIPProcessor.from_pretrained(\"./clip-lora-mimic-cxr\")\nbase_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = PeftModel.from_pretrained(base_model, \"./clip-lora-mimic-cxr\")\nmodel.to(device)\nmodel.eval()\n\n@torch.no_grad()\ndef safe_forward(model, input_ids, attention_mask, pixel_values):\n    if hasattr(model, 'base_model'):\n        base_model = model.base_model\n    else:\n        base_model = model\n    \n    text_outputs = base_model.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    vision_outputs = base_model.vision_model(pixel_values=pixel_values)\n    \n    text_embeds = base_model.text_projection(text_outputs.pooler_output)\n    image_embeds = base_model.visual_projection(vision_outputs.pooler_output)\n    \n    return text_embeds, image_embeds\n\nimage_path = \"/kaggle/input/testing1/test_img1.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\n\ncandidates = [\n    \"There is evidence of left lower lobe pneumonia.\",\n    \"The heart size is enlarged with signs of pulmonary edema.\",\n    \"Lungs are clear with no acute cardiopulmonary abnormality.\"\n]\n\ninputs = processor(\n    text=candidates,\n    images=[image] * len(candidates),\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True,\n    max_length=77\n)\n\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\ntext_embeds, image_embeds = safe_forward(\n    model,\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    pixel_values=inputs[\"pixel_values\"]\n)\n\ntext_embeds = F.normalize(text_embeds, p=2, dim=1)\nimage_embeds = F.normalize(image_embeds, p=2, dim=1)\nsimilarities = torch.matmul(image_embeds, text_embeds.T)\n\nbest_idx = similarities.argmax().item()\nprint(f\"Most similar sentence:\\n \\\"{candidates[best_idx]}\\\"\")\nprint(f\"Similarity scores: {similarities.cpu().numpy()[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T09:34:42.230749Z","iopub.execute_input":"2025-06-25T09:34:42.231399Z","iopub.status.idle":"2025-06-25T09:34:43.676685Z","shell.execute_reply.started":"2025-06-25T09:34:42.231368Z","shell.execute_reply":"2025-06-25T09:34:43.675897Z"}},"outputs":[{"name":"stdout","text":"Most similar sentence:\n \"Lungs are clear with no acute cardiopulmonary abnormality.\"\nSimilarity scores: [0.18905325 0.20127088 0.2176774 ]\n","output_type":"stream"}],"execution_count":101}]}